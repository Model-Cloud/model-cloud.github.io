{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"ModelCloud Documentation","text":"<p>Welcome to ModelCloud \u2013 the decentralized AI inference network that makes large language models accessible, affordable, and globally distributed.</p>"},{"location":"#what-is-modelcloud","title":"What is ModelCloud?","text":"<p>ModelCloud is a revolutionary platform that transforms how AI inference works by creating a decentralized marketplace where:</p> <ul> <li>Developers get instant access to powerful language models through our OpenAI-compatible API</li> <li>GPU owners monetize their hardware by running inference nodes and earning per token</li> <li>Everyone benefits from reduced latency, lower costs, and censorship-resistant AI</li> </ul>"},{"location":"#core-features","title":"Core Features","text":""},{"location":"#openai-compatible-api","title":"\ud83d\ude80 OpenAI-Compatible API","text":"<p>Drop-in replacement for OpenAI's API with zero code changes required. Start using models like Llama, Mistral, and more through familiar endpoints.</p> <pre><code>curl -X POST \"https://api.modelcloud.ai/v1/chat/completions\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"model\": \"llama-3.2-3b\", \"messages\": [{\"role\": \"user\", \"content\": \"Hello!\"}]}'\n</code></pre>"},{"location":"#intelligent-routing","title":"\u26a1 Intelligent Routing","text":"<p>Our smart routing engine automatically selects the fastest available node based on: - Geographic proximity - Node performance metrics - Real-time latency measurements - Model availability</p>"},{"location":"#decentralized-marketplace","title":"\ud83d\udcb0 Decentralized Marketplace","text":"<ul> <li>For Developers: Pay only for successful inference, with transparent per-token pricing</li> <li>For Node Operators: Earn cryptocurrency for every token generated on your hardware</li> <li>Global Network: Redundancy and reliability through distributed infrastructure</li> </ul>"},{"location":"#privacy-security","title":"\ud83d\udd12 Privacy &amp; Security","text":"<ul> <li>No data logging or storage</li> <li>End-to-end encryption</li> <li>Censorship-resistant architecture</li> <li>Your prompts never leave the inference process</li> </ul>"},{"location":"#getting-started","title":"Getting Started","text":""},{"location":"#for-developers","title":"For Developers","text":"<ol> <li>Quick Start Guide - Get up and running in 5 minutes</li> <li>Chat Completions API - Complete API reference</li> <li>Routing System - Understanding how requests are routed</li> </ol>"},{"location":"#for-node-operators","title":"For Node Operators","text":"<ol> <li>Run a Node - Setup your GPU for inference</li> <li>Payout System - How earnings work</li> <li>Hardware Requirements - Minimum specs and recommendations</li> </ol>"},{"location":"#why-choose-modelcloud","title":"Why Choose ModelCloud?","text":"Traditional AI APIs ModelCloud Centralized servers Distributed globally Fixed pricing Market-driven rates Limited model selection Growing ecosystem Potential censorship Censorship-resistant Single point of failure Redundant infrastructure Data retention policies Zero data storage"},{"location":"#use-cases","title":"Use Cases","text":"<ul> <li>Chatbots &amp; Assistants: Build conversational AI with low latency</li> <li>Content Generation: Scale content creation with distributed compute</li> <li>Code Generation: Integrate AI coding assistants</li> <li>Research &amp; Experimentation: Access diverse models cost-effectively</li> <li>Edge Computing: Reduce latency with geographically distributed nodes</li> </ul>"},{"location":"#network-statistics","title":"Network Statistics","text":"<p>ModelCloud's decentralized network provides: - Global Coverage: Nodes across 6 continents - Model Variety: 15+ popular open-source models - Uptime: 99.9% network availability - Performance: Average response time under 2 seconds</p>"},{"location":"#recent-updates","title":"Recent Updates","text":"<p>\ud83d\udcd6 Blog: OpenAI-Compatible Router - Deep dive into our routing architecture</p> <p>\ud83d\udcd6 Blog: Running a Node - Complete guide to joining the network</p> <p>\ud83d\udcd6 Blog: Routing &amp; Latency - How we optimize for speed</p>"},{"location":"#community-support","title":"Community &amp; Support","text":"<ul> <li>GitHub - Open source components</li> <li>Discord - Join our community (coming soon)</li> <li>Documentation - You're reading it!</li> <li>Status Page - Real-time network health (coming soon)</li> </ul>"},{"location":"#ready-to-build","title":"Ready to Build?","text":"<p>Get Started{ .md-button .md-button--primary } Run a Node{ .md-button } API Reference{ .md-button }</p> <p>Need help? Check out our Quick Start Guide or explore the API documentation. Ready to monetize your GPU? Learn how to run a node.</p>"},{"location":"quickstart/","title":"Quickstart","text":"<p>This guide helps you make your first request to ModelCloud.</p>"},{"location":"quickstart/#1-get-an-api-key","title":"1. Get an API Key","text":"<p>Sign up and retrieve your API key from the Console.</p>"},{"location":"quickstart/#2-make-a-chat-completion-request","title":"2. Make a Chat Completion request","text":"<pre><code>curl https://api.modelcloud.example.com/v1/chat/completions \\\n  -H \"Authorization: Bearer $MC_API_KEY\" \\\n  -H \"Content-Type\": \"application/json\" \\\n  -d '{\n    \"model\": \"TinyLlama 1.1B\",\n    \"messages\": [\n      { \"role\": \"developer\", \"content\": \"You are a helpful assistant.\" },\n      { \"role\": \"user\", \"content\": \"Hello!\" }\n    ]\n  }'\n</code></pre>"},{"location":"quickstart/#3-stream-responses-optional","title":"3. Stream responses (optional)","text":"<p>Use <code>stream: true</code> to receive tokens as they are generated.</p>"},{"location":"quickstart/#4-next-steps","title":"4. Next steps","text":"<ul> <li>Review limits and timeouts</li> <li>Explore model catalog &amp; BYO models</li> <li>Integrate SDKs (coming soon)</li> </ul>"},{"location":"routing/","title":"Routing","text":"<p>ModelCloud routes requests to nodes based on:</p> <ul> <li>Health checks and recent success rates</li> <li>Observed latency (P50/P95)</li> <li>Model availability and capacity</li> </ul> <p>Routing adapts in real time with dynamic rebalancing to keep latency low and throughput high.</p>"},{"location":"api/chat-completions/","title":"API: Chat Completions","text":"<p>Endpoint: <code>POST /v1/chat/completions</code></p> <p>Request body (partial):</p> <pre><code>{\n  \"model\": \"TinyLlama 1.1B\",\n  \"messages\": [\n    { \"role\": \"developer\", \"content\": \"You are a helpful assistant.\" },\n    { \"role\": \"user\", \"content\": \"Hello!\" }\n  ],\n  \"temperature\": 0.8,\n  \"stream\": false\n}\n</code></pre> <p>Response (summary):</p> <pre><code>{\n  \"id\": \"...\",\n  \"object\": \"chat.completion\",\n  \"created\": 1720000000,\n  \"model\": \"TinyLlama 1.1B\",\n  \"choices\": [\n    {\n      \"index\": 0,\n      \"message\": { \"role\": \"assistant\", \"content\": \"...\" },\n      \"finish_reason\": \"stop\"\n    }\n  ]\n}\n</code></pre> <p>Notes: - Compatible with OpenAI-style chat schema - Use streaming for low-latency token delivery - Timeouts depend on model and router configuration</p>"},{"location":"blog/2025/08/09/why-we-built-an-openai-compatible-router-on-a-decentralized-network/","title":"Why we built an OpenAI-compatible router on a decentralized network","text":"<p>Compatibility lowers switching costs; routing lowers latency and cost.</p> <p>Most teams standardized on OpenAI\u2019s Chat Completions API. We designed ModelCloud to be drop\u2011in compatible, so you can point existing clients at our router and immediately benefit from a global, decentralized GPU network.</p>"},{"location":"blog/2025/08/09/why-we-built-an-openai-compatible-router-on-a-decentralized-network/#the-problem-were-solving","title":"The problem we\u2019re solving","text":"<p>Running AI inference is expensive and spiky. Centralized providers bundle compute, routing and pricing in a way that makes it hard to optimize latency and cost. Meanwhile, there\u2019s growing supply of idle GPUs \u2013 at the edge, in labs, in small datacenters \u2013 that can serve traffic competitively if we can route requests intelligently.</p>"},{"location":"blog/2025/08/09/why-we-built-an-openai-compatible-router-on-a-decentralized-network/#our-approach","title":"Our approach","text":"<ul> <li>API compatibility: we support <code>POST /v1/chat/completions</code>, messages arrays, temperature, and common parameters, so migrations are trivial.</li> <li>Decentralized supply: model nodes authenticate with HQ and sync deployments. Nodes run model backends (e.g. llama.cpp), and pull orders via gRPC.</li> <li>Intelligent routing: the router matches requests to nodes by recent health, observed latency (P50/P95), and model availability. If a node degrades, traffic rebalances automatically.</li> <li>Transparent economics: node operators earn per usage; buyers see clear per\u20111K token pricing and can bring their own models where licensing allows.</li> </ul>"},{"location":"blog/2025/08/09/why-we-built-an-openai-compatible-router-on-a-decentralized-network/#how-it-works-high-level","title":"How it works (high level)","text":"<ol> <li>Your client calls our router:    <code>bash    curl https://api.modelcloud.example.com/v1/chat/completions \\      -H \"Authorization: Bearer $MC_API_KEY\" \\      -H \"Content-Type: application/json\" \\      -d '{        \"model\": \"TinyLlama 1.1B\",        \"messages\": [          { \"role\": \"developer\", \"content\": \"You are a helpful assistant.\" },          { \"role\": \"user\", \"content\": \"Hello!\" }        ]      }'</code></li> <li>The router enqueues an order for the requested model.</li> <li>A model node pulls the order via gRPC (<code>TakeOrder</code>), runs inference (e.g. llama.cpp), then completes via <code>CompleteOrder</code>.</li> <li>The router returns a standard Chat Completions response.</li> </ol>"},{"location":"blog/2025/08/09/why-we-built-an-openai-compatible-router-on-a-decentralized-network/#why-compatibility-matters","title":"Why compatibility matters","text":"<ul> <li>Zero\u2011friction trials: point staging at ModelCloud, no client rewrites.</li> <li>Tooling works: observability and SDKs built for OpenAI still operate.</li> <li>Future\u2011proof: you can A/B providers behind a single client.</li> </ul>"},{"location":"blog/2025/08/09/why-we-built-an-openai-compatible-router-on-a-decentralized-network/#why-decentralization-wins","title":"Why decentralization wins","text":"<ul> <li>Lower latency: route to the nearest healthy node.</li> <li>Better prices: tap into diverse supply and market dynamics.</li> <li>Resilience: avoid single\u2011provider outages; auto\u2011rebalance under load.</li> </ul>"},{"location":"blog/2025/08/09/why-we-built-an-openai-compatible-router-on-a-decentralized-network/#whats-next","title":"What\u2019s next","text":"<ul> <li>Streaming tokens with backpressure</li> <li>Model selection hints &amp; routing policies</li> <li>Node reputation and dynamic rewards</li> </ul> <p>If you\u2019re interested in trying ModelCloud or running a node, head to the Console to get started.</p>"},{"location":"blog/2025/08/09/why-we-built-an-openai-compatible-router-on-a-decentralized-network/#appendix-supported-parameters-preview","title":"Appendix: Supported parameters (preview)","text":"<ul> <li><code>model</code>: string (required)</li> <li><code>messages</code>: array of role/content objects (required)</li> <li><code>temperature</code>, <code>top_p</code>, <code>top_k</code>, <code>repeat_penalty</code>, <code>seed</code></li> <li><code>stream</code>: boolean</li> </ul> <p>Note: exact parameter set depends on the underlying model backend and will continue to expand.</p>"},{"location":"blog/2025/08/09/inside-the-matching-engine-routing-for-low-p95-latency/","title":"Inside the matching engine: routing for low P95 latency","text":"<p>We combine health signals, observed latency, and model availability to route traffic to the best nodes.</p>"},{"location":"blog/2025/08/09/how-to-run-a-modelcloud-node-and-start-earning/","title":"How to run a ModelCloud node and start earning","text":"<ol> <li>Register a node in the Console and copy your node ID/secret.</li> <li>Install the node agent on your machine or server.</li> <li>Authenticate with HQ and sync deployed models.</li> <li>Pass health checks (latency, uptime) and start earning.</li> </ol>"},{"location":"nodes/payouts/","title":"Payouts","text":"<p>Earnings are calculated based on usage (tokens/second) adjusted for latency and reliability. Payouts are processed weekly.</p> <ul> <li>Transparent dashboards for usage and rewards</li> <li>Reputation improves allocation and effective rates</li> <li>Fraud and abuse monitoring protects the network</li> </ul>"},{"location":"nodes/run-a-node/","title":"Run a Node","text":"<p>ModelCloud nodes earn by serving inference requests. High-level steps:</p> <ol> <li>Register a node in the Console and copy your node ID/secret.</li> <li>Install the node agent on your machine or server.</li> <li>Authenticate with HQ and sync deployed models.</li> <li>Pass health checks (latency, uptime) and start earning.</li> </ol>"},{"location":"nodes/run-a-node/#requirements","title":"Requirements","text":"<ul> <li>Recent GPU drivers and sufficient VRAM for your models</li> <li>Stable network and power</li> <li>Optional: llama.cpp server if using that runner</li> </ul>"},{"location":"nodes/run-a-node/#tips","title":"Tips","text":"<ul> <li>Keep latency low: pin models, avoid swapping, prefer NVMe storage</li> <li>Maintain uptime for higher reputation and rewards</li> </ul>"},{"location":"blog/archive/2025/","title":"2025","text":""},{"location":"blog/category/launch/","title":"Launch","text":""}]}